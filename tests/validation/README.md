# üß™ Guia de Valida√ß√£o de Modelos - Aurora EchoTales

Este guia orienta a valida√ß√£o individual de cada modelo de IA antes da integra√ß√£o completa.

---

## üöÄ Instala√ß√£o

### Passo 1: Criar Ambiente Virtual

```powershell
# Na raiz do projeto
python -m venv venv
.\venv\Scripts\Activate.ps1
```

### Passo 2: Instalar Depend√™ncias

```powershell
# Instalar PyTorch com CUDA
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# Instalar Llama-cpp-Python com CUDA
pip install llama-cpp-python --prefer-binary --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121

# Instalar Hugging Face Hub
pip install huggingface-hub

# Instalar demais depend√™ncias
pip install -r requirements-validation.txt

# Instalar Bark (TTS)
pip install git+https://github.com/suno-ai/bark.git
```

### Passo 3: Verificar Ambiente

```powershell
python check_environment.py
```

**Output esperado:**
```
‚úÖ PyTorch
‚úÖ Transformers (Hugging Face)
‚úÖ CUDA dispon√≠vel
   GPU: NVIDIA GeForce RTX 4060
   VRAM: 8.0 GB

üéâ Ambiente pronto para valida√ß√£o!
```

---

## üß™ Executando os Testes

### Ordem Recomendada

Execute os testes na seguinte ordem (do mais leve ao mais pesado):

#### 1Ô∏è‚É£ An√°lise de Emo√ß√£o de Texto (mais r√°pido)

```powershell
python test_text_emotion.py
```

**Tempo estimado**: 30 segundos  
**VRAM**: ~0.5GB  
**O que valida**: Detec√ß√£o de emo√ß√µes em texto (joy, sadness, anger, etc.)
**Teste VRAM**: 0.5GB

---

#### 2Ô∏è‚É£ Whisper Speech-to-Text

```powershell
python test_whisper_stt.py
```

**Tempo estimado**: 1-2 minutos  
**VRAM**: ~2GB  
**O que valida**: Transcri√ß√£o de √°udio para texto
**Teste VRAM**: 1.60GB

---

#### 3Ô∏è‚É£ Emotion2Vec (An√°lise de √Åudio)

```powershell
python test_emotion2vec.py
```

**Tempo estimado**: 1 minuto  
**VRAM**: ~1-2GB  
**O que valida**: Detec√ß√£o de emo√ß√µes em √°udio
**Teste VRAM**: 1.30GB (poss√≠vel aumento ap√≥s fine-tuning)

---

#### 4Ô∏è‚É£ LLaMA 3.1 8B (Gera√ß√£o de Hist√≥rias)

```powershell
python test_llama2_story.py
```

**Tempo estimado**: 3-5 minutos  
**VRAM**: ~3.5GB  
**O que valida**: Gera√ß√£o de narrativas interativas
**Teste VRAM**: 4.90GB

‚ö†Ô∏è **Importante**: Este teste baixar√° o modelo (~13GB). Primeira execu√ß√£o demora mais.

---

#### 5Ô∏è‚É£ Riffusion (Gera√ß√£o de M√∫sica)

```powershell
python test_riffusion.py
```

**Tempo estimado**: 5-10 minutos  
**VRAM**: ~3GB  
**O que valida**: Gera√ß√£o de m√∫sica procedural via prompts
**Teste VRAM**: 3GB

üéµ **Output**: Arquivos PNG de espectrogramas em `test_riffusion_output_*.png`

---

#### 6Ô∏è‚É£ XTTS v2 (TTS Narrador)

```powershell
python test_bark_tts.py
```

**Tempo estimado**: 5-10 minutos  
**VRAM**: ~4GB  
**O que valida**: S√≠ntese de voz natural com emo√ß√µes
**Teste VRAM**: 2GB

üéß **Output**: Arquivos WAV em `test_bark_output_*.wav`

---

## üéØ Pr√≥ximos Passos

Ap√≥s valida√ß√£o bem-sucedida:

1. ‚úÖ Todos os testes PASS ‚Üí Prosseguir para integra√ß√£o
2. üìã Documentar quaisquer otimiza√ß√µes necess√°rias
3. üîß Implementar Module Manager
4. üîÑ Criar Orchestrator Service
5. üöÄ Integra√ß√£o incremental dos m√≥dulos

---

## üí° Dicas de Performance

### Otimiza√ß√£o de VRAM
```python
# Em cada teste, adicionar:
torch.cuda.empty_cache()
gc.collect()
```

### Quantiza√ß√£o Agressiva
```python
# Para modelos maiores:
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)
```

### Attention Slicing (Riffusion)
```python
# Reduz pico de VRAM:
pipe.enable_attention_slicing()
pipe.enable_vae_slicing()
```

---

## üìö Recursos Adicionais

- [VALIDATION_PLAN.md](../../VALIDATION_PLAN.md) - Plano completo com scripts
- [Hugging Face Docs](https://huggingface.co/docs)
- [PyTorch CUDA Guide](https://pytorch.org/docs/stable/cuda.html)
- [Bark GitHub](https://github.com/suno-ai/bark)
- [Riffusion Docs](https://www.riffusion.com/about)
