import whisper
import time
import torch
import numpy as np
from test_llama2_story import get_memory_usage

def test_whisper():
    print("=" * 60)
    print("üéôÔ∏è  TESTE: Whisper Small - Speech-to-Text")
    print("=" * 60)
    
    ram_before, vram_before = get_memory_usage()
    print(f"\nüìä Mem√≥ria Antes: RAM {ram_before:.2f}GB | VRAM {vram_before:.2f}GB")
    
    # Carregar modelo
    print("\n‚è≥ Carregando Whisper Small...")
    start = time.time()
    model = whisper.load_model("small", device="cuda")
    load_time = time.time() - start
    
    ram_after, vram_after = get_memory_usage()
    print(f"‚úÖ Carregado em {load_time:.2f}s")
    print(f"üìä Mem√≥ria: RAM {ram_after:.2f}GB (+{ram_after-ram_before:.2f}) | VRAM {vram_after:.2f}GB (+{vram_after-vram_before:.2f})")
    
    # Teste com √°udio sint√©tico (simula√ß√£o)
    print("\nüéµ Gerando √°udio de teste...")
    # Simula 10s de √°udio (16kHz, mono)
    sample_rate = 16000
    duration = 10
    audio = np.random.randn(sample_rate * duration).astype(np.float32)
    
    # Transcri√ß√£o
    print("\nüìù Transcrevendo...")
    start = time.time()
    result = model.transcribe(audio, language="en")
    transcribe_time = time.time() - start
    
    print(f"‚è±Ô∏è  Tempo: {transcribe_time:.2f}s para {duration}s de √°udio")
    print(f"üìÑ Texto detectado: {result['text']}")
    
    # Crit√©rios
    if vram_after > 3:
        print("‚ö†Ô∏è  AVISO: VRAM alto para Whisper")
    if transcribe_time > 15:
        print("‚ö†Ô∏è  AVISO: Tempo de transcri√ß√£o lento")
    
    print("\n‚úÖ TESTE CONCLU√çDO")
    return True

if __name__ == "__main__":
    test_whisper()